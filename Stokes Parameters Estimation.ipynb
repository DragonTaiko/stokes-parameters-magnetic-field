{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Impots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, glob\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib.gridspec as gridspec\n",
    "from time import time\n",
    "\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (22,16)\n",
    "plt.rcParams['figure.dpi'] = 200\n",
    "\n",
    "#gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "#tf.config.experimental.set_memory_growth(gpus[0], True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_mean_absolute_percentage_error(true, pred, mean=False):\n",
    "    true = np.array(true)\n",
    "    pred = np.array(pred)\n",
    "    if true.ndim == 1:\n",
    "        true = true.reshape(1,true.shape[0])\n",
    "        pred = pred.reshape(1,pred.shape[0])\n",
    "    wmape = np.abs(pred-true).sum(axis=1) / np.abs(true).sum(axis=1) * 100\n",
    "    return wmape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_train_test_set(data_path, test_path, train_rows=None, test_rows=None):\n",
    "    if train_rows:\n",
    "        df = pd.read_csv(data_path, nrows=train_rows)\n",
    "    else:\n",
    "        df = pd.read_csv(data_path)\n",
    "        \n",
    "    if test_rows:\n",
    "        test_df = pd.read_csv(test_path, nrows=test_rows)\n",
    "    else:\n",
    "        test_df = pd.read_csv(test_path)\n",
    "    return df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_resolution(df, test_df, all_stk_dim, resolution, xcols, ycols, shift=0):\n",
    "    mid_point = int(all_stk_dim/2) + shift\n",
    "    stks = []\n",
    "    for s in y_cols:\n",
    "        for i in range(int(mid_point - np.floor(resolution/2)), int(mid_point + np.ceil(resolution/2))):\n",
    "            stks.append(s+str(i))\n",
    "    stk_dim = int(len(stks) / 4)\n",
    "    x = df[xcols]\n",
    "    y = df[stks]\n",
    "\n",
    "    xt = test_df[xcols]\n",
    "    yt = test_df[stks]\n",
    "    \n",
    "    return x,y,xt,yt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaler(train, kind='standard', transforms=[]):\n",
    "    if kind == 'standard':\n",
    "        scaler = StandardScaler()\n",
    "    elif kind == 'minmax':\n",
    "        scaler = MinMaxScaler()\n",
    "    elif kind == 'maxabs':\n",
    "        scaler = MaxAbsScaler()\n",
    "    elif kind == 'quantile':\n",
    "        scaler = QuantileTransformer()\n",
    "        \n",
    "    scaler.fit(train)\n",
    "    results = []\n",
    "    for transform in transforms:\n",
    "        results.append(scaler.transform(transform))\n",
    "    \n",
    "    return scaler, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fully_connected_model(hidden_layers, activation, output, l2):\n",
    "    model = tf.keras.Sequential()\n",
    "    for neurons in hidden_layers:\n",
    "        model.add(tf.keras.layers.Dense(neurons, activation=activation, \n",
    "                                        kernel_regularizer=tf.keras.regularizers.l2(l2)))\n",
    "        \n",
    "    #model.add(tf.keras.layers.BatchNormalization())\n",
    "    #model.add(tf.keras.layers.Dropout(0.5))\n",
    "    model.add(tf.keras.layers.Dense(output))\n",
    "    \n",
    "    return model\n",
    "\n",
    "def model_train(x_train, y_train, model, loss, epochs, patience, batch_size, optimizer, \n",
    "                verbose, scheduler=None):\n",
    "    callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=patience)\n",
    "    callbacks = [callback]\n",
    "    if scheduler:\n",
    "        scheduler_callback = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
    "        callbacks.append(scheduler_callback)\n",
    "            \n",
    "    model.compile(loss=loss, optimizer=optimizer, metrics=['mae', 'mape', 'mse'])\n",
    "    history = model.fit(x_train, y_train, epochs=epochs, verbose=verbose, batch_size=batch_size, \n",
    "                        validation_split=0.15, callbacks=callbacks)\n",
    "    \n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_result_csv(y_test, y_pred):\n",
    "    e = ['I', 'Q', 'U', 'V']\n",
    "    data = {'stokes I MSE': [], 'stokes Q MSE': [], 'stokes U MSE': [], 'stokes V MSE': [], 'stokes I WMAPE': [], 'stokes Q WMAPE': [], 'stokes U WMAPE': [], 'stokes V WMAPE': [], 'Amplitude stokes I':[], 'Amplitude stokes Q':[], 'Amplitude stokes U':[], 'Amplitude stokes V':[]}\n",
    "    for i in range(4):\n",
    "        wmape = weighted_mean_absolute_percentage_error(y_test[:,i*32:(i+1)*32], y_pred[:,i*32:(i+1)*32])\n",
    "        mse = tf.keras.metrics.mean_squared_error(y_test[:,i*32:(i+1)*32], y_pred[:,i*32:(i+1)*32]).numpy()\n",
    "        data['stokes ' + e[i] + ' MSE'] = mse\n",
    "        data['stokes ' + e[i] + ' WMAPE'] = wmape\n",
    "        if i != 0:\n",
    "            data['Amplitude stokes ' + e[i]] = np.max(np.abs(y_test[:,i*32:(i+1)*32]), axis=1)\n",
    "        else:\n",
    "            data['Amplitude stokes ' + e[i]] = np.min(np.abs(y_test[:,i*32:(i+1)*32]), axis=1)\n",
    "\n",
    "    data = pd.DataFrame(data)\n",
    "    data['WMAPE'] = (data['stokes I WMAPE'] + data['stokes Q WMAPE'] + data['stokes U WMAPE'] + data['stokes V WMAPE'])/4\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../fe6311/'#'../data/'\n",
    "results_path = '../results/'\n",
    "size_dataset = int(2e6)\n",
    "test_size_dataset = int(2e5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_high, test_df_high = load_train_test_set(data_path+'cossam_train_data_high.csv', \n",
    "                                  data_path+'cossam_test_data_high.csv', \n",
    "                                  size_dataset, test_size_dataset)\n",
    "print('Train shape', df_high.shape)\n",
    "print('Test shape', test_df_high.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['fmag', 'incl', 'alpha', 'beta', 'gamma', 'y2', 'y3', 'phase'] #vrot\n",
    "y_cols = ['stki_' , 'stkq_', 'stku_', 'stkv_']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_high, y_train_high, x_test_high, y_test_high = load_resolution(df_high, test_df_high, int((df_high.shape[1] - 11)/5), 32, cols, y_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_high = x_train_high.to_numpy()\n",
    "x_test_high = x_test_high.to_numpy()\n",
    "y_train_high = y_train_high.to_numpy()\n",
    "y_test_high = y_test_high.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_scalerX, (x_train_high_s, x_test_high_s) = scaler(x_train_high, 'maxabs', [x_train_high, x_test_high])\n",
    "high_scalerY, (y_train_high_s, y_test_high_s) = scaler(y_train_high, 'standard', [y_train_high, y_test_high])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_dataset = x_train_high.shape[0]\n",
    "opt = tf.keras.optimizers.SGD(learning_rate=1e-1, momentum=0.95, \n",
    "                              decay=1/(2 * size_dataset))\n",
    "model = fully_connected_model([4096]*7, 'relu', y_train_high_s.shape[1], 0)\n",
    "model, history = model_train(x_train_high_s, y_train_high_s, model, 'mse', 1000, 25, 1024, opt, 1)\n",
    "model.save('new results/1.3M_maxabs_standard_high.h5')\n",
    "pred = high_scalerY.inverse_transform(model.predict(x_test_high_s, batch_size=1024))\n",
    "res = make_result_csv(y_test_high, pred)\n",
    "res.to_csv('new results/1.3M_maxabs_standard_high.csv', index=False)\n",
    "res.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_history(history, 'loss', 1e-2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
