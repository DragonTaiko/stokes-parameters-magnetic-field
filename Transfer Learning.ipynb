{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, glob\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib.gridspec as gridspec\n",
    "from matplotlib.patches import Patch\n",
    "\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, QuantileTransformer, MaxAbsScaler\n",
    "\n",
    "#plt.style.use(\"dark_background\")\n",
    "plt.rcParams[\"figure.figsize\"] = (22,16)\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "sns.set(font_scale=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaler(train, kind='standard', transforms=[]):\n",
    "    if kind == 'standard':\n",
    "        scaler = StandardScaler()\n",
    "    elif kind == 'minmax':\n",
    "        scaler = MinMaxScaler()\n",
    "    elif kind == 'maxabs':\n",
    "        scaler = MaxAbsScaler()\n",
    "    elif kind == 'quantile':\n",
    "        scaler = QuantileTransformer()\n",
    "        \n",
    "    scaler.fit(train)\n",
    "    results = []\n",
    "    for transform in transforms:\n",
    "        results.append(scaler.transform(transform))\n",
    "    \n",
    "    return scaler, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_mean_absolute_percentage_error(true, pred, mean=False):\n",
    "    true = np.array(true)\n",
    "    pred = np.array(pred)\n",
    "    if true.ndim == 1:\n",
    "        true = true.reshape(1,true.shape[0])\n",
    "        pred = pred.reshape(1,pred.shape[0])\n",
    "    wmape = np.abs(pred-true).sum(axis=1) / np.abs(true).sum(axis=1) * 100\n",
    "    return wmape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_percentile_plots(y_true, y_pred, percentiles, ref_percentile, mses, mapes, wmapes, rows, size, \n",
    "                          rows_supylabel, palette='mako'):\n",
    "    c = sns.color_palette(palette)[0]\n",
    "    c2 = sns.color_palette(palette)[3]\n",
    "    props = dict(boxstyle='round', facecolor='white', alpha=0.5)\n",
    "    cols = 1\n",
    "    stks = ['I', 'Q', 'U', 'V']\n",
    "    kth = 0\n",
    "    ith_p = []\n",
    "    \n",
    "    if len(ref_percentile) != rows:\n",
    "        models = 1\n",
    "        for perc in percentiles:\n",
    "            _, ith = find_nearest(ref_percentile, np.quantile(ref_percentile, perc))\n",
    "            ith_p.append(ith)\n",
    "            \n",
    "        ref_percentile = [ref_percentile]*rows\n",
    "    \n",
    "    print(ith_p)\n",
    "    fig = plt.figure(figsize=size)\n",
    "    grid = plt.GridSpec(rows, cols)\n",
    "    \n",
    "    for i in range(rows * cols):\n",
    "        fake = fig.add_subplot(grid[i])        \n",
    "        idx = np.where(np.array(sorted(ref_percentile[i])) == ref_percentile[i][ith_p[i]])\n",
    "        p = int(np.rint((idx[0][0]+1) / len(ref_percentile[i]) * 100))\n",
    "        mse = np.round(mses[ith_p[i]], 12)\n",
    "        mape = np.round(mapes[ith_p[i]], 3)\n",
    "        wmape = np.round(wmapes[ith_p[i]], 4)\n",
    "        \n",
    "        txt = 'Percentil '+str(p)+'\\n WMAPE = '+str(wmape)+'\\n' \n",
    "        fake.set_title(txt, \n",
    "                       size=48)\n",
    "        fake.set_axis_off()\n",
    "\n",
    "        gs = gridspec.GridSpecFromSubplotSpec(1, 4, subplot_spec=grid[i])\n",
    "\n",
    "        for col in range(4):\n",
    "            y_true_col = y_true[ith_p[i]][32*col:32*(col+1)]\n",
    "            y_pred_col = []\n",
    "            for ith_model in range(models):\n",
    "                y_pred_col.append(y_pred[ith_p[0]][32*col:32*(col+1)])\n",
    "                y_pred_col.append(y_pred[ith_p[1]][32*col:32*(col+1)])\n",
    "                y_pred_col.append(y_pred[ith_p[2]][32*col:32*(col+1)])\n",
    "                \n",
    "            mape = [tf.keras.metrics.mean_absolute_percentage_error(y_true_col, y_pred_col[0]).numpy(), \n",
    "                    tf.keras.metrics.mean_absolute_percentage_error(y_true_col, y_pred_col[1]).numpy(),\n",
    "                    tf.keras.metrics.mean_absolute_percentage_error(y_true_col, y_pred_col[2]).numpy()]\n",
    "            mse = [tf.keras.metrics.mean_squared_error(y_true_col, y_pred_col[0]).numpy(), \n",
    "                    tf.keras.metrics.mean_squared_error(y_true_col, y_pred_col[1]).numpy(),\n",
    "                    tf.keras.metrics.mean_squared_error(y_true_col, y_pred_col[2]).numpy()]\n",
    "            wmape = [weighted_mean_absolute_percentage_error(y_true_col, y_pred_col[0]),\n",
    "                     weighted_mean_absolute_percentage_error(y_true_col, y_pred_col[1]),\n",
    "                     weighted_mean_absolute_percentage_error(y_true_col, y_pred_col[2])]\n",
    "            \n",
    "            text = 'MSE = '+str(np.round(mse[i],16))+'\\nWMAPE = '+str(np.round(wmape[i][0], 4))\n",
    "            ax = fig.add_subplot(gs[col])\n",
    "            \n",
    "            sns.lineplot(x=range(32), y=y_true_col, ax=ax, linewidth=6, color=c)\n",
    "            sns.lineplot(x=range(32), y=y_pred_col[i], ax=ax, linewidth=6, color=c2, linestyle='--')\n",
    "            ax.text(0.05, 0.95, text, transform=ax.transAxes, fontsize=36, verticalalignment='top', \n",
    "                    bbox=props)\n",
    "\n",
    "            if i == 0:\n",
    "                ax.set_title(f'Stokes {stks[col]}')\n",
    "                ax.set_ylabel(f'{stks[col]} / IC',labelpad=0)\n",
    "            if col == 0:\n",
    "                ax.annotate(rows_supylabel[i], xy=(0, 0.5), xytext=(-ax.yaxis.labelpad - 3, 0), \n",
    "                            xycoords=ax.yaxis.label, textcoords='offset points',\n",
    "                            size='large', ha='right', va='center', rotation='vertical')\n",
    "    fig.patch.set_facecolor('white')\n",
    "    fig.tight_layout(w_pad=5, h_pad=1)\n",
    "    p1 = Patch(facecolor=c, label='Verdad')\n",
    "    p2 = Patch(facecolor=c2, label='Predicci√≥n')\n",
    "\n",
    "    fig.legend(handles=[p1, p2], loc='lower center', bbox_to_anchor=(0.5,1), \n",
    "           ncol=4, bbox_transform=fig.transFigure, fontsize='x-large')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_nearest(array, value):\n",
    "    array = np.asarray(array)\n",
    "    idx = (np.abs(array - value)).argmin()\n",
    "    return array[idx], idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fully_connected_model(hidden_layers, activation, output, l2):\n",
    "    model = tf.keras.Sequential()\n",
    "    for neurons in hidden_layers:\n",
    "        model.add(tf.keras.layers.Dense(neurons, activation=activation, \n",
    "                                        kernel_regularizer=tf.keras.regularizers.l2(l2)))\n",
    "        \n",
    "    model.add(tf.keras.layers.Dense(output))\n",
    "    \n",
    "    return model\n",
    "\n",
    "def model_train(x_train, y_train, model, loss, epochs, patience, batch_size, optimizer, \n",
    "                verbose, scheduler=None):\n",
    "    callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=patience)\n",
    "    callbacks = [callback]\n",
    "    if scheduler:\n",
    "        scheduler_callback = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
    "        callbacks.append(scheduler_callback)\n",
    "            \n",
    "    model.compile(loss=loss, optimizer=optimizer, metrics=['mae', 'mape', 'mse'])\n",
    "    history = model.fit(x_train, y_train, epochs=epochs, verbose=verbose, batch_size=batch_size, \n",
    "                        validation_split=0.15, callbacks=callbacks)\n",
    "    \n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_train_test_set(data_path, test_path, train_rows=None, test_rows=None):\n",
    "    if train_rows:\n",
    "        df = pd.read_csv(data_path, nrows=train_rows)\n",
    "    else:\n",
    "        df = pd.read_csv(data_path)\n",
    "        \n",
    "    if test_rows:\n",
    "        test_df = pd.read_csv(test_path, nrows=test_rows)\n",
    "    else:\n",
    "        test_df = pd.read_csv(test_path)\n",
    "    return df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_resolution(df, test_df, all_stk_dim, resolution, xcols, ycols, shift=0):\n",
    "    mid_point = int(all_stk_dim/2) + shift\n",
    "    stks = []\n",
    "    for s in y_cols:\n",
    "        for i in range(int(mid_point - np.floor(resolution/2)), int(mid_point + np.ceil(resolution/2))):\n",
    "            stks.append(s+str(i))\n",
    "    stk_dim = int(len(stks) / 4)\n",
    "    x = df[xcols]\n",
    "    y = df[stks]\n",
    "\n",
    "    xt = test_df[xcols]\n",
    "    yt = test_df[stks]\n",
    "    \n",
    "    return x,y,xt,yt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../data/'\n",
    "results_path = '../results/'\n",
    "size_dataset = int(2e6)\n",
    "test_size_dataset = int(2e5)\n",
    "\n",
    "new_size_dataset = int(3.5e4)\n",
    "new_test_size_dataset = int(3.5e3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df, test_df = load_train_test_set('../fe6311/cossam_train_data_high.csv', \n",
    "                                  '../fe6311/cossam_test_data_high.csv', \n",
    "                                  size_dataset, test_size_dataset)\n",
    "\n",
    "print('Train shape', df.shape)\n",
    "print('Test shape', test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df, new_test_df = load_train_test_set(data_path+'cossam_train_data_MZS_high.csv', \n",
    "                                  data_path+'cossam_test_data_MZS_high.csv', \n",
    "                                  new_size_dataset, new_test_size_dataset)\n",
    "\n",
    "print('Train shape', new_df.shape)\n",
    "print('Test shape', new_test_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['fmag', 'incl', 'alpha', 'beta', 'gamma', 'y2', 'y3', 'phase']\n",
    "y_cols = ['stki_' , 'stkq_', 'stku_', 'stkv_']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_new_train_normal, y_new_train_normal, x_new_test_normal, y_new_test_normal = load_resolution(new_df,new_test_df, int((new_df.shape[1] - 11)/5), 32, cols, y_cols, shift=0)\n",
    "x_train_normal, y_train_normal, x_test_normal, y_test_normal = load_resolution(df, test_df, int((df.shape[1] - 11)/5), 32, cols, y_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_normal = x_train_normal.to_numpy()\n",
    "x_test_normal = x_test_normal.to_numpy()\n",
    "y_train_normal = y_train_normal.to_numpy()\n",
    "y_test_normal = y_test_normal.to_numpy()\n",
    "\n",
    "x_new_train_normal = x_new_train_normal.to_numpy()\n",
    "x_new_test_normal = x_new_test_normal.to_numpy()\n",
    "y_new_train_normal = y_new_train_normal.to_numpy()\n",
    "y_new_test_normal = y_new_test_normal.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_scalerX, (x_train_s, x_test_s) = scaler(x_train_normal, 'maxabs', \n",
    "    [x_train_normal, x_test_normal])\n",
    "normal_scalerY, (y_train_s, y_test_s) = scaler(y_train_normal, 'standard', \n",
    "    [y_train_normal, y_test_normal])\n",
    "\n",
    "new_normal_scalerX, (x_new_train_s, x_new_test_s) = scaler(x_new_train_normal, 'maxabs', [x_new_train_normal, x_new_test_normal])\n",
    "new_normal_scalerY, (y_new_train_s, y_new_test_s) = scaler(y_new_train_normal,  'standard', [y_new_train_normal, y_new_test_normal])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model('new results/1.3M_maxabs_standard_high.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model_stats(model, x_train_s, y_train_normal, x_test_s, \n",
    "                      y_test_normal, [normal_scalerY])\n",
    "_, y_test_pred_base, _, _, _, mse_test_base, _, mape_test_base, wmape_test_base = results\n",
    "#error_stats_by_stokes(y_new_test_normal, y_test_pred_untrained)\n",
    "print('Original')\n",
    "print('---------------------------------')\n",
    "print('WMAPE:', wmape_test_base.mean(), 'MAPE:', mape_test_base.mean(), ' MSE:', mse_test_base.mean(), '\\n')\n",
    "\n",
    "errors = error_stats_by_stokes(y_test_normal, y_test_pred_base)\n",
    "for i in range(4):\n",
    "    print(y_cols[i][:-1], errors['wmape mean'][i+1], '+-', errors['wmape std'][i+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_percentile_plots(y_test_normal, y_test_pred_base, [0.8, 0.9, 0.99], wmape_test_base,\n",
    "                      mse_test_base, mape_test_base, wmape_test_base, 3, (48,24), ['']*3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Last Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_model = tf.keras.models.clone_model(model)\n",
    "tmp_model.set_weights(model.get_weights())\n",
    "\n",
    "x = tmp_model.layers[-2].output\n",
    "predictions = tf.keras.layers.Dense(128, name='output')(x)\n",
    "transfer_model = tf.keras.models.Model(inputs=tmp_model.input, outputs=predictions)\n",
    "\n",
    "for i in range(len(transfer_model.layers) - 1):\n",
    "    transfer_model.layers[i].trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = tf.keras.optimizers.SGD(learning_rate=1e-1, momentum=0.95, decay=1/(2*new_size_dataset))\n",
    "transfer_model, history = model_train(x_new_train_s, y_new_train_s, transfer_model, 'mse', 1000, 25, 1024, opt, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuning_model = tf.keras.models.clone_model(transfer_model)\n",
    "finetuning_model.set_weights(transfer_model.get_weights())\n",
    "\n",
    "s = 6\n",
    "for i in range(len(finetuning_model.layers) - s):\n",
    "    finetuning_model.layers[i+s].trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = tf.keras.optimizers.SGD(learning_rate=0.5e-1, momentum=0.95, decay=1/(4*new_size_dataset))\n",
    "finetuning_model, history = model_train(x_new_train_s, y_new_train_s, finetuning_model, 'mse', 1000, 25, 1024, opt, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model_stats(finetuning_model, x_new_train_s, y_new_train_normal, x_new_test_s, \n",
    "                      y_new_test_normal, [new_normal_scalerY])\n",
    "_, y_test_pred_ft, _, _, _, mse_test_ft, _, mape_test_ft, wmape_test_ft = results\n",
    "\n",
    "print('Fine Tuning')\n",
    "print('---------------------------------')\n",
    "print('WMAPE:', wmape_test_ft.mean(), 'MAPE:', mape_test_ft.mean(), ' MSE:', mse_test_ft.mean(), '\\n')\n",
    "\n",
    "errors = error_stats_by_stokes(y_new_test_normal, y_test_pred_ft)\n",
    "for i in range(4):\n",
    "    print(y_cols[i][:-1], errors['wmape mean'][i+1], '+-', errors['wmape std'][i+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wmapes = [np.zeros(len(y_new_test_normal)), np.zeros(len(y_new_test_normal)), np.zeros(len(y_new_test_normal)), np.zeros(len(y_new_test_normal)), np.zeros(len(y_new_test_normal))]\n",
    "mses = [np.zeros(len(y_new_test_normal)), np.zeros(len(y_new_test_normal)), np.zeros(len(y_new_test_normal)), np.zeros(len(y_new_test_normal))]\n",
    "for i in range(4):\n",
    "    wmapes[i] += weighted_mean_absolute_percentage_error(y_new_test_normal[:,i*32:(i+1)*32], y_test_pred_ft[:,i*32:(i+1)*32])\n",
    "    mses[i] += tf.keras.metrics.mean_squared_error(y_new_test_normal[:,i*32:(i+1)*32], y_test_pred_ft[:,i*32:(i+1)*32]).numpy()\n",
    "wmapes[4] = (wmapes[0] + wmapes[1] + wmapes[2] + wmapes[3])/4\n",
    "\n",
    "resdf = pd.DataFrame({'WMAPE': wmapes[4], 'WMAPE stokes I': wmapes[0], 'WMAPE stokes Q': wmapes[1], 'WMAPE stokes U': wmapes[2], 'WMAPE stokes V': wmapes[3], 'stokes I MSE': mses[0], 'stokes Q MSE': mses[1], 'stokes U MSE': mses[2], 'stokes V MSE': mses[3]})\n",
    "resdf.to_csv('new results/35k_1.3M_ft_high.csv', index=False)\n",
    "\n",
    "finetuning_model.save('new results/35k_finetuning_1.3M_high.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_percentile_plots(y_new_test_normal, y_test_pred_ft, [0.85, 0.95, 0.99], wmape_test_ft,\n",
    "                      mse_test_ft, mape_test_ft, wmape_test_ft, 3, (48,24), ['']*3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
