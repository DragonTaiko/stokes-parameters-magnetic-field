{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler, MaxAbsScaler\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (32,16)\n",
    "plt.rcParams['figure.dpi'] = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_mean_absolute_percentage_error(true, pred, mean=False):\n",
    "    true = np.array(true)\n",
    "    pred = np.array(pred)\n",
    "    if true.ndim == 1:\n",
    "        true = true.reshape(1,true.shape[0])\n",
    "        pred = pred.reshape(1,pred.shape[0])\n",
    "    wmape = np.abs(pred-true).sum(axis=1) / np.abs(true).sum(axis=1) * 100\n",
    "    return wmape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Particle:\n",
    "    def __init__(self, pos, bounds):\n",
    "        self.pbest_fit = float('inf')\n",
    "        self.bounds = bounds\n",
    "        self.position = np.array(pos)\n",
    "        self.velocity = np.array([0]*self.position.shape[0])\n",
    "        self.pbest_pos = np.copy(self.position)\n",
    "                \n",
    "    def fly(self):\n",
    "        for i in range(self.position.shape[0]):\n",
    "            new_pos = np.copy(self.position[i] + self.velocity[i])\n",
    "            if new_pos > self.bounds[1] or new_pos < self.bounds[0]:\n",
    "                new_pos = np.random.uniform(self.bounds[0], self.bounds[1])\n",
    "            self.position[i] = new_pos\n",
    "            \n",
    "class PSO:\n",
    "    def __init__(self, n_particles, iters, bounds, dims, y_true, phases, W=0.7, C1=1, C2=1, \n",
    "                 C1_decay=1e-1, C2_decay=1e-1, mode='random', weights=[1,1,1,1]):\n",
    "        self.W, self.C1, self.C2 = W, C1, C2\n",
    "        self.C1_init, self.C2_init = C1, C2\n",
    "        self.C1_decay, self.C2_decay = C1_decay, C2_decay\n",
    "        self.iters = iters\n",
    "        self.n_particles = n_particles\n",
    "        self.particles = []\n",
    "        self.y_true = y_true\n",
    "        self.phases = phases\n",
    "        self.gbest_hist = []\n",
    "        self.weights = weights\n",
    "        \n",
    "        self.gbest_fit = float('inf')\n",
    "        self.gbest_pos = np.array(self.init(bounds, dims))\n",
    "        \n",
    "        if mode == 'equal' or mode == 'equal_random':\n",
    "            self.init_values_equal(bounds, dims)\n",
    "        for i in range(n_particles):\n",
    "            if mode == 'equal_random':\n",
    "                r = np.random.uniform(0, 1)\n",
    "                if r <= 0.5:\n",
    "                    mode = 'random'\n",
    "                else:\n",
    "                    mode = 'equal'\n",
    "            self.particles.append(Particle(self.init(bounds, dims, mode), bounds))\n",
    "        \n",
    "    def set_C1(self, iteration):\n",
    "        self.C1 = self.C1_init * (1 / (1 + self.C1_decay * iteration))\n",
    "        \n",
    "    def set_C2(self, iteration):\n",
    "        self.C2 = self.C2_init * (1 + self.C2_decay * iteration)\n",
    "    \n",
    "    def init_values_equal(self, bounds, dims):\n",
    "        values = np.linspace(bounds[0], bounds[1], 8)\n",
    "        comb = list(combinations_with_replacement(values, dims))\n",
    "        r = np.array(random.sample(comb, self.n_particles))\n",
    "        self.r = r\n",
    "    \n",
    "    def init(self, bounds, dims, mode='random', i=0):\n",
    "        if mode == 'random':\n",
    "            return [np.random.uniform(bounds[0], bounds[1]) for _ in range(dims)]\n",
    "        elif mode == 'equal':\n",
    "            return self.r[i]\n",
    "            \n",
    "                      \n",
    "    def calculate_fitness(self):\n",
    "        positions = []\n",
    "        for p in self.particles:\n",
    "            positions.append(list(np.copy(p.position)) + self.phases)\n",
    "        positions = np.array(positions)\n",
    "        \n",
    "        y_preds = model.predict(positions, verbose=0, batch_size=2048)\n",
    "        errors = np.array(tf.keras.metrics.mean_squared_error(self.y_true, y_preds))\n",
    "        return errors\n",
    "    \n",
    "    def calculate_fitness2(self):\n",
    "        positions = []\n",
    "        for p in self.particles:\n",
    "            pos = np.copy(p.position)\n",
    "            for ph in self.phases:\n",
    "                positions.append(list(pos)+ list([ph]))\n",
    "        positions = np.array(positions)\n",
    "        y_preds = model.predict(positions, verbose=0, batch_size=2048)\n",
    "        y_preds = y_preds.reshape(self.y_true.shape[0], self.y_true.shape[1])\n",
    "        errors = np.zeros(self.n_particles)\n",
    "        for i in range(len(self.phases)):\n",
    "            y_true_tmp, y_preds_tmp = self.y_true[:,i*128:(i+1)*128], y_preds[:,i*128:(i+1)*128]\n",
    "            errors += tf.keras.metrics.mean_squared_error(y_true_tmp[:,:32], y_preds_tmp[:,:32])*self.weights[0] + tf.keras.metrics.mean_squared_error(y_true_tmp[:,32:64], y_preds_tmp[:,32:64])*self.weights[1] + tf.keras.metrics.mean_squared_error(y_true_tmp[:,64:96], y_preds_tmp[:,64:96])*self.weights[2] + tf.keras.metrics.mean_squared_error(y_true_tmp[:,96:], y_preds_tmp[:,96:])*self.weights[3]\n",
    "            errors /= len(self.weights)\n",
    "        errors /= len(self.phases)\n",
    "        #print(y_preds.shape)\n",
    "        \n",
    "        errors = np.array(tf.keras.metrics.mean_squared_error(self.y_true, y_preds))\n",
    "        return errors\n",
    "    \n",
    "    def set_pbest_gbest(self, iteration):\n",
    "        fits = self.calculate_fitness2()\n",
    "        i = 0\n",
    "        for fit in fits:\n",
    "            if fit < self.particles[i].pbest_fit:\n",
    "                self.particles[i].pbest_fit = fit\n",
    "                self.particles[i].pbest_pos = np.copy(self.particles[i].position)\n",
    "            if fit < self.gbest_fit:\n",
    "                self.gbest_fit = fit\n",
    "                self.gbest_pos = np.copy(self.particles[i].position)\n",
    "                self.gbest_ith = iteration\n",
    "            i += 1\n",
    "                     \n",
    "    def fly(self, W ,C1, C2):\n",
    "        for p in self.particles:\n",
    "            velocity = (W*p.velocity) + (C1*np.random.random())*(p.pbest_pos - p.position) + (C2*np.random.random())*(self.gbest_pos - p.position)\n",
    "            p.velocity = np.copy(velocity)\n",
    "            p.fly()    \n",
    "        \n",
    "    def fit(self, tol=None, stuck=None, verbose=-1):\n",
    "        i = 0\n",
    "        stuck_flag = False\n",
    "        stuck_counter = 0\n",
    "        while i < self.iters and not stuck_flag: \n",
    "            gbest_prev = self.gbest_fit\n",
    "            self.fly(W=self.W, C1=self.C1, C2=self.C2)\n",
    "            gbest_prev = self.gbest_fit\n",
    "            self.set_pbest_gbest(i)\n",
    "            self.gbest_hist.append(self.gbest_fit)\n",
    "            #self.set_C1(i)\n",
    "            #self.set_C2(i)\n",
    "            i += 1\n",
    "            if stuck:\n",
    "                if gbest_prev == self.gbest_fit:\n",
    "                    stuck_counter += 1\n",
    "                else:\n",
    "                    stuck_counter = 0\n",
    "                    \n",
    "                if stuck_counter == stuck:\n",
    "                    stuck_flag = True\n",
    "                    \n",
    "            if tol:\n",
    "                if self.gbest_fit <= tol:\n",
    "                    stuck_flag = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_result_csv(y_test, y_pred):\n",
    "    e = ['I', 'Q', 'U', 'V']\n",
    "    data = {'stokes I MSE': [], 'stokes Q MSE': [], 'stokes U MSE': [], 'stokes V MSE': [], 'stokes I WMAPE': [], 'stokes Q WMAPE': [], 'stokes U WMAPE': [], 'stokes V WMAPE': [], 'Amplitude stokes I':[], 'Amplitude stokes Q':[], 'Amplitude stokes U':[], 'Amplitude stokes V':[]}\n",
    "    for i in range(4):\n",
    "        wmape = weighted_mean_absolute_percentage_error(y_test[:,i*32:(i+1)*32], y_pred[:,i*32:(i+1)*32])\n",
    "        mse = tf.keras.metrics.mean_squared_error(y_test[:,i*32:(i+1)*32], y_pred[:,i*32:(i+1)*32]).numpy()\n",
    "        data['stokes ' + e[i] + ' MSE'] = mse\n",
    "        data['stokes ' + e[i] + ' WMAPE'] = wmape\n",
    "        if i != 0:\n",
    "            data['Amplitude stokes ' + e[i]] = np.max(np.abs(y_test[:,i*32:(i+1)*32]), axis=1)\n",
    "        else:\n",
    "            data['Amplitude stokes ' + e[i]] = np.min(np.abs(y_test[:,i*32:(i+1)*32]), axis=1)\n",
    "\n",
    "    data = pd.DataFrame(data)\n",
    "    data['WMAPE'] = (data['stokes I WMAPE'] + data['stokes Q WMAPE'] + data['stokes U WMAPE'] + data['stokes V WMAPE'])/4\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_MSE_WMAPE_error(y_test, y_pred):\n",
    "    e = ['I', 'Q', 'U', 'V']\n",
    "    data = {'stokes I MSE': [], 'stokes Q MSE': [], 'stokes U MSE': [], 'stokes V MSE': [], 'stokes I WMAPE': [], 'stokes Q WMAPE': [], 'stokes U WMAPE': [], 'stokes V WMAPE': [], 'WMAPE': [], 'Amplitude stokes I':[], 'Amplitude stokes Q':[], 'Amplitude stokes U':[], 'Amplitude stokes V':[]}\n",
    "    for i in range(4):\n",
    "        wmape = weighted_mean_absolute_percentage_error(y_test[:,i*32:(i+1)*32], y_pred[:,i*32:(i+1)*32])\n",
    "        mse = tf.keras.metrics.mean_squared_error(y_test[:,i*32:(i+1)*32], y_pred[:,i*32:(i+1)*32]).numpy()\n",
    "        data['stokes ' + e[i] + ' MSE'] = mse\n",
    "        data['stokes ' + e[i] + ' WMAPE'] = wmape\n",
    "        if i != 0:\n",
    "            data['Amplitude stokes ' + e[i]] = np.max(np.abs(y_test[:,i*32:(i+1)*32]), axis=1)\n",
    "        else:\n",
    "            data['Amplitude stokes ' + e[i]] = np.min(np.abs(y_test[:,i*32:(i+1)*32]), axis=1)\n",
    "\n",
    "    data['WMAPE'] = (data['stokes I WMAPE'] + data['stokes Q WMAPE'] + data['stokes U WMAPE'] + data['stokes V WMAPE']) / 4\n",
    "    return pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_inversions_csv(x, y, input_scaler, output_scaler, solutions, phases, times, errors, model):\n",
    "    d = {'fmag': [], 'incl': [], 'alpha': [], 'beta': [], 'gamma': [], 'x2': [], 'x3': [], \n",
    "         'fmag pred': [], 'incl pred': [], 'alpha pred': [], 'beta pred': [], 'gamma pred': [], \n",
    "         'x2 pred': [], 'x3 pred': [], 'time': [], 'error': []}\n",
    "\n",
    "    for i in range(phases.shape[1]):\n",
    "        d['stokes I MSE ph'+str(i)] = []\n",
    "        d['stokes Q MSE ph'+str(i)] = []\n",
    "        d['stokes U MSE ph'+str(i)] = []\n",
    "        d['stokes V MSE ph'+str(i)] = []\n",
    "\n",
    "        d['stokes I WMAPE ph'+str(i)] = []\n",
    "        d['stokes Q WMAPE ph'+str(i)] = []\n",
    "        d['stokes U WMAPE ph'+str(i)] = []\n",
    "        d['stokes V WMAPE ph'+str(i)] = []\n",
    "        d['WMAPE ph'+str(i)] = []\n",
    "    \n",
    "    final_solutions = input_scaler.inverse_transform(solutions)\n",
    "\n",
    "    for i in range(len(x)):\n",
    "        input_data = np.tile(solutions[i][:7], phases.shape[1]).reshape(phases.shape[1], 7)\n",
    "        input_data = np.column_stack([input_data, phases[i]])\n",
    "        y_pred = output_scaler.inverse_transform(model.predict(input_data, batch_size=phases.shape[1], verbose=0))\n",
    "        y_test = y[i].reshape(phases.shape[1], 128)\n",
    "\n",
    "        tmp = make_MSE_WMAPE_error(y_test, y_pred)\n",
    "        for ph in range(phases.shape[1]):\n",
    "            d['stokes I MSE ph'+str(ph)].append(tmp.iloc[ph]['stokes I MSE'])\n",
    "            d['stokes Q MSE ph'+str(ph)].append(tmp.iloc[ph]['stokes Q MSE'])\n",
    "            d['stokes U MSE ph'+str(ph)].append(tmp.iloc[ph]['stokes U MSE'])\n",
    "            d['stokes V MSE ph'+str(ph)].append(tmp.iloc[ph]['stokes V MSE'])\n",
    "\n",
    "            d['stokes I WMAPE ph'+str(ph)].append(tmp.iloc[ph]['stokes I WMAPE'])\n",
    "            d['stokes Q WMAPE ph'+str(ph)].append(tmp.iloc[ph]['stokes Q WMAPE'])\n",
    "            d['stokes U WMAPE ph'+str(ph)].append(tmp.iloc[ph]['stokes U WMAPE'])\n",
    "            d['stokes V WMAPE ph'+str(ph)].append(tmp.iloc[ph]['stokes V WMAPE'])\n",
    "\n",
    "            d['WMAPE ph'+str(ph)].append(tmp.iloc[ph]['WMAPE'])\n",
    "\n",
    "        d['fmag'].append(x[i][0])\n",
    "        d['incl'].append(x[i][1])\n",
    "        d['alpha'].append(x[i][2])\n",
    "        d['beta'].append(x[i][3])\n",
    "        d['gamma'].append(x[i][4])\n",
    "        d['x2'].append(x[i][5])\n",
    "        d['x3'].append(x[i][6])\n",
    "\n",
    "        d['fmag pred'].append(final_solutions[i][0])\n",
    "        d['incl pred'].append(final_solutions[i][1])\n",
    "        d['alpha pred'].append(final_solutions[i][2])\n",
    "        d['beta pred'].append(final_solutions[i][3])\n",
    "        d['gamma pred'].append(final_solutions[i][4])\n",
    "        d['x2 pred'].append(final_solutions[i][5])\n",
    "        d['x3 pred'].append(final_solutions[i][6])\n",
    "\n",
    "        d['time'].append(times[i])\n",
    "        d['error'].append(errors[i])\n",
    "\n",
    "    rdf = pd.DataFrame(d)\n",
    "    return rdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../data/'\n",
    "results_path = 'new results/'\n",
    "\n",
    "size_dataset = int(3.5e6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../fe6311/cossam_train_data_high.csv', nrows=size_dataset)\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('../data/cossam_data_test_inversions_6311_high.csv')\n",
    "print(test_df.shape)\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_stk_dim = int((df.shape[1] - 11)/5)\n",
    "resolution = 32\n",
    "mid_point = int(all_stk_dim/2)\n",
    "start = int(mid_point - np.floor(resolution/2))\n",
    "end = int(mid_point + np.ceil(resolution/2))\n",
    "\n",
    "cols = ['fmag', 'incl', 'alpha', 'beta', 'gamma', 'y2', 'y3', 'phase']\n",
    "stks = [] \n",
    "y_cols = ['stki_', 'stkq_', 'stku_','stkv_']\n",
    "for s in y_cols:\n",
    "    for i in range(start, end):\n",
    "        stks.append(s+str(i))\n",
    "stk_dim = int(len(stks) / 4)\n",
    "cols = ['fmag', 'incl', 'alpha', 'beta', 'gamma', 'y2', 'y3']\n",
    "x = df[cols + ['phase']]\n",
    "y = df[stks]\n",
    "\n",
    "\n",
    "phases = 7\n",
    "stks = []\n",
    "#\n",
    "y_cols = ['stki_ph', 'stkq_ph', 'stku_ph', 'stkv_ph']\n",
    "for ph in range(phases):\n",
    "    ycs = [e+str(ph) for e in y_cols]\n",
    "    for s in ycs:\n",
    "        for i in range(start, end):\n",
    "            stks.append(s+'_'+str(i-1))\n",
    "\n",
    "xt = test_df[cols + ['phase_ph'+str(i) for i in range(phases)]]\n",
    "yt = test_df[stks]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x.to_numpy()\n",
    "x_test = xt.to_numpy()\n",
    "y_train = y.to_numpy()\n",
    "y_test = yt.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scalerX = StandardScaler()\n",
    "x_train_s = scalerX.fit_transform(x_train[:, :-1])\n",
    "phase_scaler = StandardScaler()\n",
    "phase_s = phase_scaler.fit_transform(x_train[:, -1].reshape(len(x_train), 1))\n",
    "x_train_s = np.concatenate((x_train_s, phase_s), axis=1)\n",
    "\n",
    "x_test_s = scalerX.transform(x_test[:, :7])\n",
    "for i in range(phases):\n",
    "    phase_s = phase_scaler.transform(x_test[:, 7+i:8+i])\n",
    "    x_test_s = np.concatenate((x_test_s, phase_s), axis=1)\n",
    "\n",
    "scalerY = MaxAbsScaler()\n",
    "y_train_s = scalerY.fit_transform(y_train)\n",
    "\n",
    "stk_dim = resolution*4\n",
    "tmp = []\n",
    "for i in range(phases):\n",
    "    tmp.append(scalerY.transform(y_test[:,i*stk_dim:(i+1)*stk_dim]))\n",
    "y_test_s = np.concatenate(tmp, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model('new results/1M_standard_maxabs_verylow.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recover Magnetic Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 2048\n",
    "phases = 7\n",
    "n = 100\n",
    "all_phases = []\n",
    "for j in range(10):\n",
    "    times = []\n",
    "    errors = []\n",
    "    solutions = []\n",
    "    print('------------------------')\n",
    "    print('Start', j, 'run')\n",
    "    print('------------------------')\n",
    "    for i in range(n):\n",
    "        s = time.time()\n",
    "        tmp = np.copy(y_test_s[i])\n",
    "        pso = PSO(n_particles=k, iters=50, bounds=(x_train_s.min(), x_train_s.max()), dims=7, \n",
    "              y_true=np.tile(tmp, k).reshape(k, stk_dim*phases), phases=list(x_test_s[i][-phases:]), \n",
    "              mode='random', C1=1, C2=1, weights=[1,1,1,1])\n",
    "        pso.fit()\n",
    "        e = time.time()\n",
    "        times.append(e - s)\n",
    "        errors.append(pso.gbest_fit)\n",
    "        solutions.append(pso.gbest_pos)\n",
    "        all_phases.append(pso.phases)\n",
    "        if i % 25 == 0:\n",
    "            print(i, 'iterations')\n",
    "    print(n, 'iterations')\n",
    "        \n",
    "        \n",
    "\n",
    "    times = np.array(times)\n",
    "    errors = np.array(errors)\n",
    "    solutions = np.array(solutions)\n",
    "    rdf = make_inversions_csv(x_test[:n], y_test[:n], scalerX, scalerY, solutions, np.array(all_phases), times, errors, model)\n",
    "    rdf.to_csv('new results/pso_inversions_2048p_50iters_'+str(phases)+'phases_'+str(j)+'_high.csv', index=False)\n",
    "    rdf.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
